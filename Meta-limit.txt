### META: BLOCK AI In meta and Xtags:  ! ###
...........................................................

* No AI:
<meta name="robots" content="noai">
<meta name="robots" content="noai, nosummary,">

* To not allow AI to use any images on the page:

<meta name="robots" content="noimageai">

**To place both directives:

<meta name="robots" content="noai, noimageai">

###Alternative:
<meta name="googlebot" content="noml">

* And request that Microsoft does not include a page in their Bing, does not use the page for training, with:

<meta name="bingbot" content="noml">

* You might also request that OpenAI, for example, does not use the page for machine learning with:

<meta name="gptbot" content="noml">

#ALL togheter:
<meta name="robots" content="noai, nosummary,noimageai">

## Footer:
# Add text in footer, may help?:
# from: https://postopen.org/content-protection-project/

#### Add this text to your web site terms of service and any copyright statements related to your content:

Any copying, including ephemeral copying, for the purpose of training an artificial intelligence (AI), large language model (LLM), 
machine learning system or neural network is prohibited.

......................................................

##Prevents Bing's bot from using the content for AI-related purposes, such as training language models.
Prevents Bing's bot from using the content for AI purposes.  Be careful, can sometimes prevent indexing! 

        <meta name="bingbot" content="nocache">
		
		X-Robots-Tag:bingbot" content="nocache
***

# CCBOT
<meta name="CCBot" content="nofollow">

......................................................
# LLM TRAINER:

H# lML Element ( Robots Meta Tag)
* fROM: https://www.searchenginejournal.com/new-rules-will-block-ai-training-bots/532348/
The following are the proposed meta robots directives:

<meta name=”robots” content=”DisallowAITraining”>

## The following are the proposed * Robots.txt rules:
DisallowAITraining – instructs the parser to not use the data for AI training language model.

User-agent: AITraining /
Disallow: /

<meta name="tdm-reservation" content="1">


# LLM, training:

250408:
<meta name="robots" content="noAIPreferenceVocabularyTerm">

............................
## ADD: X-Robots-Tag HTTP header:
DeviantArt covers how to set the X-Robots-Tag header (which also has other uses to help control how search engines crawl your site) to express an opt-out.
On Apache httpd (I know, I’m old school) it’s something like this:


Header Set X-Robots-Tag "noai"

#  X tags:
Header set X-Robots-Tag: noai"

Header set X-Robots-Tag: noimageai"

Header set X-Robots-Tag: noml"

Header set X-Robots-Tag: nofollow, noml"

Header set X-Robots-Tag: tdm-reservation: 1"

OR:
Header set X-Robots-Tag: DisallowAITraining"

Header set X-Robots-Tag "noai, noimageai"

Header set tdm-reservation "1"
Header set X-Robots-Tag:noAIPreferenceVocabularyTerm"

Header set X-Robots-Tag "noai, noimageai, nosummary"

#ALl togheter:

Header set X-Robots-Tag "noai, noimageai, nosummary, noml"

......................................
#  TDM HEAD:

<!DOCTYPE html>
<html>
<head>
  <meta name="robots" content="noai, noimageai">
  <meta name="tdm-reservation" content="1">
  <meta name="tdm-policy" content="">
</head>
</html>
--------------------

## privacy opt out for servers ****
This is the first site to include the new SPC meta tag and X-Robots-Tag header 
for a privacy opt-out that works like Global Privacy Control but for servers. 
Basically you have legally enforceable rights in your personal information, blogs have personal information, 
but regular GPC only works from your browser (client) to company on the server. This goes the other way, and sends a legally enforceable* 
*yes, I know, this has not yet been tested in court, but give it a minute, we’re just getting started hereprivacy signal from a personal blog on the server 
to an AI scraper on the client side.

So the new header on here is:

Header set X-Robots-Tag: noai, noimageai, SPC"

Header set X-Robots-Tag “noai, noimageai”

## VCL powered Fastly service:

set resp.http.x-robots-tag = "noai";

resp.headers.set("x-robots-tag", "noai");

:::::::::::::::::::::::::::::::::::::
## Nginx servers, add this to your configuration file:
add_header X-Robots-Tag “noai, noimageai”;

:::::::::::::::::::::::::::::::::::::
## Express.js (Node.js) applications, modify the response headers:
app.use((req, res, next) => {
res.setHeader(“X-Robots-Tag”, “noai, noimageai”);
next();
});

:::::::::::::::::::::::::::::::::::::
#IN .htaccess put: Then they only can get one visit/per IP! Works great, have it on my blogs:

<IfModule mod_rewrite.c>
MaxConnPerIP 2
</IfModule>

# Another way:
MaxClients < number-of-connections> 

::::::::::::::::::::::::::::::::::::::::::::::::::
#### RATE LIMIT: 
## I use this on .htaccess 1 - my own:)

<IfModule mod_rewrite.c>
RewriteCond %{REQUEST_URI} !^/images/
RewriteRule ^(.*)$ - [E=RATE_LIMIT:1]
Header set Retry-After "77550" env=RATE_LIMIT</IfModule>
<IfModule ratelimit_module>
SetOutputFilter RATE_LIMIT
SetEnv rate-limit 100
SetEnv rate-initial-burst 200

OR:
</IfModule>
<IfModule mod_rewrite.c>
RewriteCond %{REQUEST_URI} !^/Post/
RewriteRule ^(.*)$ - [E=RATE_LIMIT:1]
Header set Retry-After "5" env=RATE_LIMIT</ifModule>


::::::::::::::::::::::::::::::::::::::::::::::::::
# NGINX-Rate Limit:

limit_req_zone $binary_remote_addr zone=req_limit_per_ip:10m rate=10r/s;


::::::::::::::::::::::::::::::::::::::::::::::::::
### Enable Rate Limiting on Your Server For Apache (Using mod_evasive):
Install mod_evasive (assuming it’s not installed):
BASH | sudo apt-get install libapache2-mod-evasive
Configure rate limits in /etc/apache2/mods-available/evasive.conf
DOSHashTableSize 3097
DOSPageCount 5
DOSSiteCount 50

DOSBlockingPeriod 600

Configure rate limits in /etc/apache2/mods-available/evasive.conf
BASH | sudo systemctl restart apache2
Enable Rate Limiting on Your Ser


### Enable Rate Limiting on Your Server For Nginx (Using limit_req_zone):
Nginx configuration (nginx.conf):
http {
limit_req_zone $binary_remote_addr zone=one:10m rate=1r/s;
}
server {
location / {
limit_req zone=one burst=5;
}
}

Restart Nginx
BASH | sudo systemctl restart nginx

::::::::::::::::::::::::::::::::::::::::::::::::::
##Honeypot Traps: Use hidden links to detect and block AI scrapers.
Honeypot traps work by placing hidden links or form fields on your website 
that humans won’t see or click, but scrapers will. If a bot interacts with them, you can block its IP or take other actions.

* 
How to Set Up a Honeypot Trap:
Add a Hidden Honeypot Link by placing this hidden link in your HTML:
<a href=”/trap-page” class=”honeypot”>Hidden Link</a>
<style>.honeypot { display: none; }</style>
Humans won’t see it due to display: none;.
Bots may still follow it, exposing themselves.
Create a Trap Page (trap-page.html):
Log visits to this page to identify scrapers (php example):
<?php
$ip = $_SERVER[‘REMOTE_ADDR’];
$file = ‘honeypot_log.txt’;
file_put_contents($file, “$ip\n”, FILE_APPEND);
?>
<html>
<head><meta name=”robots” content=”noindex, nofollow”></head>
<body>
Nothing to see here.
</body>
</html>
Logs suspicious IPs in honeypot_log.txt.
Prevents indexing so search engines ignore it.

::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
## Update Your Terms of Service.
Clearly state that AI scraping is prohibited in your terms of service document for the website.
NOTE: This is an example only, make sure you use legal advice to determine your own verbiage needed.

Example Verbiage:
“Unauthorized scraping, data extraction, or use of automated tools (including AI models, bots, and crawlers) 
to access, store, or repurpose content from this site is strictly prohibited. Any violation may result in legal action, IP bans, and further enforcement measures.”

### 
Use CSS Hidden Elements – Place text in display: none; sections that only appear in raw HTML

::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
Updated: 250727
